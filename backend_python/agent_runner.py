# agent_runner.py
import aiosqlite
import memory
import re
import os
import tools
import tiktoken
from ollama import AsyncClient
from functools import wraps

def session_guard(func):
    """Garantit que chaque agent dispose d'un espace de travail isol√©."""
    @wraps(func)
    async def wrapper(self, text_content: str, session_id: int | None, on_token_callback):
        print(f"üõ°Ô∏è [Session Guard] Isolation de la session {session_id or 'Nouvelle'}")
        return await func(self, text_content, session_id, on_token_callback)
    return wrapper


class AgentRunner:
    def __init__(self):
        self.encoder = tiktoken.get_encoding("cl100k_base")
        self.max_tokens = 5000  # Limite avant compaction du prompt
        
        # Client LLM d√©di√© aux t√¢ches administratives (r√©sum√© et extraction)
        self.admin_client = AsyncClient(host=os.getenv("URL_SERVER_OLLAMA"))
        self.admin_model = "llama3.1:8b" 
        
    def count_tokens(self, messages: list) -> int:
        """Compte les jetons d'un historique complet."""
        text = " ".join([str(m["content"]) for m in messages])
        return len(self.encoder.encode(text))

    async def _context_window_guard(self, history: list) -> list:
        """
        G√©n√®re un prompt compress√© (Head + R√©sum√© + Tail) √Ä LA VOL√âE.
        Ne modifie AUCUNE base de donn√©es pour pr√©server l'UI.
        """
        print("üßπ [Context Guard] Fen√™tre pleine. Compaction en m√©moire...")
        
        if len(history) <= 4:
            return history
            
        head = history[:2]   # Les 2 premiers (souvent le d√©but de conversation)
        tail = history[-2:]  # Les 2 derniers (le contexte imm√©diat)
        middle = history[2:-2] # Le corps √† compresser
        
        middle_text = "\n".join([f"{m['role']}: {m['content']}" for m in middle])
        
        # 1. Extraction des faits (Pre-Compaction Memory Flush)
        print("üíæ [Context Guard] Extraction des faits vers le long terme...")
        flush_prompt = (
            f"Analyse cet historique et liste uniquement les faits nouveaux et importants "
            f"concernant l'utilisateur. Sois tr√®s concis, sous forme de tirets. "
            f"S'il n'y a rien d'important, r√©ponds 'AUCUN'.\n\nHistorique:\n{middle_text}"
        )
        res_facts = await self.admin_client.chat(model=self.admin_model, messages=[{"role": "user", "content": flush_prompt}])
        facts = res_facts.message.content.strip()
        
        if "aucun" not in facts.lower() and len(facts) > 5:
            # On sauvegarde les faits dans la m√©moire longue (qui est souveraine)
            with open("memory/MEMORY.md", "a", encoding="utf-8") as f:
                f.write(f"\n{facts}\n")
            await memory.sync_memory_md()
            print("‚úÖ [Context Guard] Faits persist√©s dans MEMORY.md et index√©s.")

        # 2. Algorithme de compaction (pour le prompt LLM uniquement)
        print("üìâ [Context Guard] Cr√©ation du r√©sum√© pour le prompt...")
        compact_prompt = f"R√©sume cette partie de la conversation en 3 phrases maximum.\n\nHistorique:\n{middle_text}"
        res_compact = await self.admin_client.chat(model=self.admin_model, messages=[{"role": "user", "content": compact_prompt}])
        summary = res_compact.message.content.strip()
        
        compressed_history = head + [{"role": "system", "content": f"R√âSUM√â DES √âCHANGES PR√âC√âDENTS: {summary}"}] + tail
        
        print("‚úÖ [Context Guard] Compaction m√©moire termin√©e.")
        # On retourne la liste compress√©e pour l'envoyer au LLM
        return compressed_history
    async def process_multimodal_chat(self, prompt: str, image_b64: str | None,image_path: str|None, session_id: int, stream_callback):
        print(f"‚öôÔ∏è AgentRunner: Traitement multimodal pour session {session_id}")
        
        # 1. On charge l'historique directement depuis SQLite
        memory_context = []
        if session_id:
            async with aiosqlite.connect("memory/memoire.db") as db:
                async with db.execute("SELECT role, content FROM memory_chat WHERE sessionID = ? ORDER BY timestamp ASC LIMIT 20", (session_id,)) as cursor:
                    lignes = await cursor.fetchall()
                    for ligne in lignes:
                        memory_context.append({"role": ligne[0], "content": ligne[1]})
        
        # 2. Le System Prompt sp√©cifique √† la vision
        system_prompt = {
            "role": "system",
            "content": (
                "Tu es Jean-Heude, un assistant personnel intelligent et capable d'analyser des images.\n"
            )
        }
        
        # 3. Message utilisateur avec l'image
        user_message = {"role": "user", "content": prompt}
        if image_b64:
            user_message["images"] = [image_b64]
            print("üñºÔ∏è Image B64 inject√©e avec succ√®s pour Qwen3-VL.")

        full_messages = [system_prompt] + memory_context + [user_message]
        vision_model = "qwen3-vl:8b" 

        # 4. R√©cup√©ration des outils pertinents
        relevant_tools = await tools.get_relevant_tools(prompt, limit=3)

        # 5. On pr√©pare une variable pour stocker la r√©ponse finale de l'assistant
        assistant_final_text = ""

        async for token in memory.execute_agent_loop(full_messages, vision_model, available_tools=relevant_tools, mute_audio=True):
            await stream_callback(token)
            
            # On attrape les mots au vol, on ignore la pens√©e (¬∂) et l'audio
            clean_chunk = re.sub(r'\|\|AUDIO_ID:.*?\|\|', '', token)
            if not clean_chunk.startswith("¬∂"): 
                assistant_final_text += clean_chunk
        
        # 6. Sauvegarde BDD (Utilisateur ET Assistant)
        if session_id:
            async with aiosqlite.connect("memory/memoire.db") as db:
                
                # Astuce magique : On ajoute la colonne 'image' dans la table si elle n'existe pas !
                try:
                    await db.execute("ALTER TABLE memory_chat ADD COLUMN image TEXT")
                except Exception:
                    pass # Si √ßa fait une erreur, c'est que la colonne existe d√©j√†, on ignore.
                
                # NOUVEAU : On ins√®re le prompt AVEC le chemin de l'image (image_path)
                await db.execute(
                    "INSERT INTO memory_chat (role, content, timestamp, sessionID, image) VALUES (?, ?, datetime('now'), ?, ?)",
                    ("user", prompt, session_id, image_path)
                )
                
                if assistant_final_text.strip():
                    # L'assistant n'a pas d'image, donc on met None √† la fin
                    await db.execute(
                        "INSERT INTO memory_chat (role, content, timestamp, sessionID, image) VALUES (?, ?, datetime('now'), ?, ?)",
                        ("assistant", assistant_final_text.strip(), session_id, None)
                    )
                await db.commit()

    @session_guard
    async def process_chat(self, text_content: str, session_id: int | None, on_token_callback):
        async with aiosqlite.connect("memory/memoire.db") as db:
            # 1. Gestion Session
            if session_id is None:
                resume = text_content[:30] + "..."
                cursor = await db.execute(
                    "INSERT INTO historique_chat (timestamp, resume, userID) VALUES (datetime('now'), ?, ?)",
                    (resume, "noe_01")
                )
                await db.commit()
                session_id = cursor.lastrowid
                print(f"üÜï Nouvelle session cr√©√©e : ID {session_id}")

            # 2. Sauvegarde INTACTE du message (pour Svelte)
            await db.execute(
                "INSERT INTO memory_chat (role, content, timestamp, sessionID) VALUES (?, ?, datetime('now'), ?)",
                ("user", text_content, session_id)
            )
            await db.commit()
            
            # 3. R√©cup√©ration de l'historique pour le LLM
            cursor = await db.execute( 
                "SELECT role, content FROM memory_chat WHERE sessionID = ? ORDER BY timestamp DESC LIMIT 20",
                (session_id,)
            )
            lignes = await cursor.fetchall()
            contexte_message = [{"role" : m[0], "content": m[1] } for m in reversed(lignes)]

        # --- 4. D√âCLENCHEMENT DU CONTEXT GUARD (En m√©moire uniquement) ---
        current_tokens = self.count_tokens(contexte_message)
        if current_tokens > self.max_tokens:
            # On √©crase contexte_message avec la version compress√©e, MAIS la DB reste intacte !
            contexte_message = await self._context_window_guard(contexte_message)

        # 5. S√©lection du mod√®le et G√©n√©ration
        chosen_model = await memory.decide_model(text_content)
        print(f"üß† Mod√®le choisi : {chosen_model}")

        assistant_final_text = ""
        
        async for chunk in memory.chat_with_memories(contexte_message, chosen_model):
            await on_token_callback(chunk)
            clean_chunk = re.sub(r'\|\|AUDIO_ID:.*?\|\|', '', chunk)
            if not clean_chunk.startswith("¬∂"): 
                assistant_final_text += clean_chunk
        
        # 6. Sauvegarde INTACTE de la r√©ponse (pour Svelte)
        if assistant_final_text.strip():
            async with aiosqlite.connect("memory/memoire.db") as db_final:
                await db_final.execute(
                    "INSERT INTO memory_chat (role, content, timestamp, sessionID) VALUES (?, ?, datetime('now'), ?)",
                    ("assistant", assistant_final_text, session_id)
                )
                await db_final.commit()
                print("‚úÖ R√©ponse assistant sauvegard√©e dans l'UI.")

        return {"session_id": session_id, "model": chosen_model}
